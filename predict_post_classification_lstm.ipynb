{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "944f22b9-0173-47b0-b947-e7dfb4ba8a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score,classification_report, confusion_matrix,precision_score, recall_score\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b322b02-f647-4810-93b9-95bf09353c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>prefLabel</th>\n",
       "      <th>compulsion_ml</th>\n",
       "      <th>obs-com_ml</th>\n",
       "      <th>obsession_ml</th>\n",
       "      <th>Document_Keywords</th>\n",
       "      <th>compulsion_bert</th>\n",
       "      <th>obs-com_bert</th>\n",
       "      <th>obsession_bert</th>\n",
       "      <th>obsession_first_checker</th>\n",
       "      <th>obs-com_first_checker</th>\n",
       "      <th>compulsion_first_checker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up with obsessive thoughts, or more apt...</td>\n",
       "      <td>['site', 'compulsion', 'obsession', 'rash', 'p...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['grow', 'obsessive_thought', 'aptly', 'intrus...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have been thinking so much about Covid becau...</td>\n",
       "      <td>['singing', 'washing hands', 'anxiety', 'anxie...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['think', 'covid', 'job', 'concern', 'encourag...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I fed up, I send all day checking excessive ch...</td>\n",
       "      <td>['OCD', 'checking', 'Thought']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['send', 'day', 'check', 'excessive', 'checkin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I’ve noticed that I’ve been washing my hands m...</td>\n",
       "      <td>['washing hands', 'hope', 'depression', 'OCD']</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['notice', 'washing_hand', 'thing', 'trigger',...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This sounds so stupid\\nAt the moment I want to...</td>\n",
       "      <td>['Thought']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['sounds_stupid', 'moment', 'download', 'windo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  \\\n",
       "0  I grew up with obsessive thoughts, or more apt...   \n",
       "1  I have been thinking so much about Covid becau...   \n",
       "2  I fed up, I send all day checking excessive ch...   \n",
       "3  I’ve noticed that I’ve been washing my hands m...   \n",
       "4  This sounds so stupid\\nAt the moment I want to...   \n",
       "\n",
       "                                           prefLabel  compulsion_ml  \\\n",
       "0  ['site', 'compulsion', 'obsession', 'rash', 'p...              1   \n",
       "1  ['singing', 'washing hands', 'anxiety', 'anxie...              0   \n",
       "2                     ['OCD', 'checking', 'Thought']              0   \n",
       "3     ['washing hands', 'hope', 'depression', 'OCD']              1   \n",
       "4                                        ['Thought']              0   \n",
       "\n",
       "   obs-com_ml  obsession_ml  \\\n",
       "0           0             1   \n",
       "1           0             1   \n",
       "2           0             1   \n",
       "3           0             1   \n",
       "4           0             1   \n",
       "\n",
       "                                   Document_Keywords  compulsion_bert  \\\n",
       "0  ['grow', 'obsessive_thought', 'aptly', 'intrus...              0.0   \n",
       "1  ['think', 'covid', 'job', 'concern', 'encourag...              0.0   \n",
       "2  ['send', 'day', 'check', 'excessive', 'checkin...              0.0   \n",
       "3  ['notice', 'washing_hand', 'thing', 'trigger',...              0.0   \n",
       "4  ['sounds_stupid', 'moment', 'download', 'windo...              0.0   \n",
       "\n",
       "   obs-com_bert  obsession_bert  obsession_first_checker  \\\n",
       "0           0.0             1.0                      1.0   \n",
       "1           0.0             1.0                      0.0   \n",
       "2           0.0             1.0                      0.0   \n",
       "3           0.0             1.0                      1.0   \n",
       "4           0.0             1.0                      1.0   \n",
       "\n",
       "   obs-com_first_checker  compulsion_first_checker  \n",
       "0                    1.0                       1.0  \n",
       "1                    0.0                       1.0  \n",
       "2                    0.0                       1.0  \n",
       "3                    0.0                       0.0  \n",
       "4                    1.0                       1.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#post data\n",
    "data = pd.read_csv('input_data/Post_ModelPredictions_ManualChecking.csv')\n",
    "df = data.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa4088f-9cb7-4153-bc09-a345142512d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#define the process of text cleaning\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r' ',text)\n",
    "#Clean Text\n",
    "def clean_text(data):\n",
    "    # convert catacter to lowercase\n",
    "    data['clean_text']=data['post'].str.lower()\n",
    "    #remove URLS\n",
    "    data['clean_text'] = data['clean_text'].apply(lambda elem:re.sub(r\"http\\S+\", \"\", elem))\n",
    "    #remove ponctuation\n",
    "    data['clean_text'] = data['clean_text'].apply(lambda elem:re.sub(r\"[^\\w\\s]\", \"\", elem))\n",
    "    #remove\n",
    "    data['clean_text'] = data['clean_text'].apply(lambda elem:re.sub(r'/n',\"\",elem))\n",
    "    #remove degits\n",
    "    data['clean_text'] = data['clean_text'].apply(lambda elem:re.sub(r'\\d+',\"\",elem))\n",
    "    #remove emojis\n",
    "    data['clean_text'] = data['clean_text'].apply(lambda elem:deEmojify(elem))\n",
    "    #remove multiple spaces\n",
    "    data['clean_text'] = data['clean_text'].apply(lambda elem:re.sub(r'\\s+',\" \",elem))\n",
    "    #remove single caracter\n",
    "    data['clean_text'] = data['clean_text'].apply(lambda elem:re.sub(r'\\s+[a-zA-Z]\\s+',\" \",elem))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7295330-4af0-426d-9aaa-90e81437dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the process of cleaning for the train and test data\n",
    "df = clean_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1ff76c8-98d0-4962-b805-fe1c136fb2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#for the text pre-processing (text cleaning)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re #regular expression\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize # word tokenization\n",
    "from nltk.stem import PorterStemmer # word stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a7d4c8-d0e3-4870-837d-0847bb5e79db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "698b7449-ae2e-4282-ad76-159d70c22459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "  stopW=stopwords.words('english')\n",
    "  s=\"\"\n",
    "  for i in text.split():\n",
    "    if i not in stopW:\n",
    "        s=s+i+\" \"\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5aca771-f446-4437-b8a2-218f44d872d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text']=df['clean_text'].apply(lambda x:remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f87b1bd9-3ac7-4407-b650-35c6a5968193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(ch):\n",
    "  stem = PorterStemmer()\n",
    "  return \" \".join([stem.stem(i) for i in ch.split()])\n",
    "\n",
    "#apply the stem function to each row in the dataframe\n",
    "df['clean_text'] = df['clean_text'].apply(lambda x:stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "223a4959-5a4c-4de3-ac20-720f5a8312d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7297adfc-bd93-4b4c-b5b5-09948b5905ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9464\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('model/lstm_tockenizer.pkl', 'rb') as f:\n",
    "    vect = pickle.load(f)\n",
    "vocab_size = len(vect.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b149bad-c5db-4add-9b82-db234b700430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23674"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "MAX_LEN = 100\n",
    "encoded_docs_test =  vect.texts_to_sequences(X_test)\n",
    "padded_docs_test = pad_sequences(encoded_docs_test, maxlen=MAX_LEN, padding='post')\n",
    "len(padded_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2aee3d0-6966-4f1d-aff7-11cc31503a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a43f3d9f-9529-46d9-879e-5abd21da1519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 16)           151424    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100, 16)           2112      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 16)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 16)                2112      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 155,682\n",
      "Trainable params: 155,682\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "model.add(Embedding(vocab_size, output_dim=16, input_length=MAX_LEN))\n",
    "model.add(LSTM(16, return_sequences=True))  \n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(16))\n",
    "model.add(Dropout(0.5))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8bf9f36-b463-483a-adb6-7be359893163",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model/lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11daec77-cf2f-4820-a30b-da4de947734f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740/740 [==============================] - 6s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions=model.predict([padded_docs_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e000d9a6-31d4-4fa0-8835-9dfc1af27bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23674"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c897047f-a081-4b38-ac47-aff17f3345cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['compulsion_lstm_pred'] = predictions[:, 0]\n",
    "data['obsession_lstm_pred'] = predictions[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "836a6755-e26c-44e3-a9ba-73ac65eb1852",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('input_data/Post_ModelPredictions_lstm_ManualChecking.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ceff0-9f18-47c1-b87e-400fa4938f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
