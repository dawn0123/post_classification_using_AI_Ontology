{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd38007-95a6-4e7c-98ac-20e48444dc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score,classification_report, confusion_matrix,precision_score, recall_score\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6c99b2-dcd8-4fbd-a24f-4fcb4efc8d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compulsion</th>\n",
       "      <th>obs-com</th>\n",
       "      <th>obsession</th>\n",
       "      <th>enriched_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hi -\\nSo I haven't been on here since December...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Hi all, {Hoffnung} hope {espoir} {hope} you're...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hi, \\nFirst, I {Hoffnung} hope {espoir} {hope}...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hello everyone. I could really use your help r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Though it comes in many flavors, one of the mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   compulsion  obs-com  obsession  \\\n",
       "0           0        0          1   \n",
       "1           1        1          1   \n",
       "2           0        0          1   \n",
       "3           0        0          1   \n",
       "4           1        1          1   \n",
       "\n",
       "                                       enriched_post  \n",
       "0  Hi -\\nSo I haven't been on here since December...  \n",
       "1  Hi all, {Hoffnung} hope {espoir} {hope} you're...  \n",
       "2  Hi, \\nFirst, I {Hoffnung} hope {espoir} {hope}...  \n",
       "3  Hello everyone. I could really use your help r...  \n",
       "4  Though it comes in many flavors, one of the mo...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#post data\n",
    "df = pd.read_csv('processed_data/enriched.csv',usecols=['enriched_post','compulsion','obs-com','obsession'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2131dc-43bc-4cf7-bda4-4f320ff853ce",
   "metadata": {},
   "source": [
    "# **Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "895c3d61-5913-476d-b3b4-be7daaf63f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#define the process of text cleaning\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r' ',text)\n",
    "#Clean Text\n",
    "def clean_text(data):\n",
    "    # convert catacter to lowercase\n",
    "    data['clean_text']=data['enriched_post'].str.lower()\n",
    "    #remove URLS\n",
    "    data['clean_text'] = data['clean_text'].apply(lambda elem:re.sub(r\"http\\S+\", \"\", elem))\n",
    "    #remove ponctuation\n",
    "    data['clean_text'] = data['clean_text'].apply(lambda elem:re.sub(r\"[^\\w\\s]\", \"\", elem))\n",
    "    #remove\n",
    "    data['clean_text'] = data['clean_text'].apply(lambda elem:re.sub(r'/n',\"\",elem))\n",
    "    #remove degits\n",
    "    data['clean_text'] = data['clean_text'].apply(lambda elem:re.sub(r'\\d+',\"\",elem))\n",
    "    #remove emojis\n",
    "    data['clean_text'] = data['clean_text'].apply(lambda elem:deEmojify(elem))\n",
    "    #remove multiple spaces\n",
    "    data['clean_text'] = data['clean_text'].apply(lambda elem:re.sub(r'\\s+',\" \",elem))\n",
    "    #remove single caracter\n",
    "    data['clean_text'] = data['clean_text'].apply(lambda elem:re.sub(r'\\s+[a-zA-Z]\\s+',\" \",elem))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8645f99d-bef1-4d6d-b52b-dcfa40682ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the process of cleaning for the train and test data\n",
    "df = clean_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff3e93de-e037-4194-be38-e690ed065f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#for the text pre-processing (text cleaning)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re #regular expression\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize # word tokenization\n",
    "from nltk.stem import PorterStemmer # word stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e9f9198-c262-4c69-b8e4-6b6ad9804e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19ca0593-b7b8-401a-ab39-f598e9c6672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "  stopW=stopwords.words('english')\n",
    "  s=\"\"\n",
    "  for i in text.split():\n",
    "    if i not in stopW:\n",
    "        s=s+i+\" \"\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1b675d-6a72-4116-a540-a9d4dfef4dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text']=df['clean_text'].apply(lambda x:remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5505f151-fe9c-4b17-87bc-7306e5ff0ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(ch):\n",
    "  stem = PorterStemmer()\n",
    "  return \" \".join([stem.stem(i) for i in ch.split()])\n",
    "\n",
    "#apply the stem function to each row in the dataframe\n",
    "df['clean_text'] = df['clean_text'].apply(lambda x:stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61f0a3fe-cae5-40c6-8894-51ba183d7a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = df['clean_text'], df[['obsession']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e37f2988-99f3-4d8d-9d47-b843a1a0f4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5235,), (5235, 1), (1309,), (1309, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23574a4f-04e0-4ad5-8842-3e058c5cbf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21242\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "num_words = 2000\n",
    "vect=Tokenizer(num_words=num_words)\n",
    "vect.fit_on_texts(X_train)\n",
    "vocab_size = len(vect.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a547b0bd-e654-4d23-b08e-92641d152595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5235, 150)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "encoded_docs_train = vect.texts_to_sequences(X_train)\n",
    "MAX_LEN = 150\n",
    "padded_docs_train = pad_sequences(encoded_docs_train, maxlen=MAX_LEN, padding='pre')\n",
    "padded_docs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d80128e-5e79-472d-8940-1c986d0e7204",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs_test =  vect.texts_to_sequences(X_test)\n",
    "padded_docs_test = pad_sequences(encoded_docs_test, maxlen=MAX_LEN, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02bb9c57-8842-419c-8c42-f01fa29dd881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e876a43-6f81-4a8e-ae73-a4466cb9c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    # Configuring the parameters\n",
    "    model.add(Embedding(num_words, output_dim=16, input_length=MAX_LEN))\n",
    "    model.add(LSTM(16, return_sequences=True))  \n",
    "    # Adding a dropout layer\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(8))\n",
    "    # model.add(Dropout(0.5))\n",
    "    model.add(Dense(4))\n",
    "    # model.add(Dropout(0.5))\n",
    "    # Adding a dense output layer with sigmoid activation\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca5ec77b-f57b-46c3-8641-7aec733b013e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 150, 16)           32000     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 150, 16)           2112      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 150, 16)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 8)                 800       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,953\n",
      "Trainable params: 34,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n",
      "262/262 [==============================] - 18s 58ms/step - loss: 0.5331 - acc: 0.7650 - val_loss: 0.4219 - val_acc: 0.8176 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "262/262 [==============================] - 15s 56ms/step - loss: 0.3509 - acc: 0.8553 - val_loss: 0.3574 - val_acc: 0.8663 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "262/262 [==============================] - 15s 57ms/step - loss: 0.3091 - acc: 0.8794 - val_loss: 0.4040 - val_acc: 0.7947 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "262/262 [==============================] - 15s 57ms/step - loss: 0.3020 - acc: 0.8723 - val_loss: 0.3514 - val_acc: 0.8548 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "262/262 [==============================] - 15s 56ms/step - loss: 0.2820 - acc: 0.8997 - val_loss: 0.3585 - val_acc: 0.8586 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "262/262 [==============================] - 15s 56ms/step - loss: 0.2643 - acc: 0.9081 - val_loss: 0.3580 - val_acc: 0.8758 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "262/262 [==============================] - 15s 58ms/step - loss: 0.2361 - acc: 0.9186 - val_loss: 0.3414 - val_acc: 0.8701 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "262/262 [==============================] - 14s 55ms/step - loss: 0.2152 - acc: 0.9243 - val_loss: 0.3203 - val_acc: 0.8758 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "262/262 [==============================] - 14s 55ms/step - loss: 0.1964 - acc: 0.9341 - val_loss: 0.3545 - val_acc: 0.8730 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "262/262 [==============================] - 14s 55ms/step - loss: 0.1870 - acc: 0.9348 - val_loss: 0.3514 - val_acc: 0.8596 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "262/262 [==============================] - 14s 55ms/step - loss: 0.1798 - acc: 0.9398 - val_loss: 0.3535 - val_acc: 0.8758 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "262/262 [==============================] - 14s 55ms/step - loss: 0.1763 - acc: 0.9372 - val_loss: 0.3293 - val_acc: 0.8730 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "262/262 [==============================] - 14s 55ms/step - loss: 0.1553 - acc: 0.9518 - val_loss: 0.3403 - val_acc: 0.8777 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "262/262 [==============================] - 14s 55ms/step - loss: 0.1385 - acc: 0.9558 - val_loss: 0.3774 - val_acc: 0.8739 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "262/262 [==============================] - 14s 55ms/step - loss: 0.1255 - acc: 0.9618 - val_loss: 0.3883 - val_acc: 0.8691 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "262/262 [==============================] - 15s 56ms/step - loss: 0.1259 - acc: 0.9604 - val_loss: 0.3821 - val_acc: 0.8653 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "262/262 [==============================] - 15s 56ms/step - loss: 0.1101 - acc: 0.9678 - val_loss: 0.4195 - val_acc: 0.8682 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "262/262 [==============================] - 15s 57ms/step - loss: 0.1003 - acc: 0.9718 - val_loss: 0.4171 - val_acc: 0.8615 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "262/262 [==============================] - 15s 56ms/step - loss: 0.0823 - acc: 0.9778 - val_loss: 0.4489 - val_acc: 0.8577 - lr: 2.0000e-04\n",
      "Epoch 20/1000\n",
      "262/262 [==============================] - 15s 56ms/step - loss: 0.0711 - acc: 0.9809 - val_loss: 0.4681 - val_acc: 0.8586 - lr: 2.0000e-04\n",
      "Epoch 21/1000\n",
      "262/262 [==============================] - 15s 56ms/step - loss: 0.0687 - acc: 0.9816 - val_loss: 0.4927 - val_acc: 0.8596 - lr: 2.0000e-04\n",
      "Epoch 22/1000\n",
      "262/262 [==============================] - 15s 58ms/step - loss: 0.0658 - acc: 0.9816 - val_loss: 0.4970 - val_acc: 0.8586 - lr: 2.0000e-04\n",
      "Epoch 23/1000\n",
      "262/262 [==============================] - 15s 56ms/step - loss: 0.0635 - acc: 0.9847 - val_loss: 0.5087 - val_acc: 0.8596 - lr: 2.0000e-04\n",
      "Epoch 24/1000\n",
      "262/262 [==============================] - 15s 56ms/step - loss: 0.0603 - acc: 0.9838 - val_loss: 0.5317 - val_acc: 0.8481 - lr: 2.0000e-04\n",
      "Epoch 25/1000\n",
      "262/262 [==============================] - 14s 55ms/step - loss: 0.0597 - acc: 0.9840 - val_loss: 0.5259 - val_acc: 0.8586 - lr: 2.0000e-04\n",
      "Epoch 26/1000\n",
      "262/262 [==============================] - 15s 56ms/step - loss: 0.0544 - acc: 0.9881 - val_loss: 0.5608 - val_acc: 0.8548 - lr: 2.0000e-04\n",
      "Epoch 27/1000\n",
      "262/262 [==============================] - 15s 56ms/step - loss: 0.0531 - acc: 0.9878 - val_loss: 0.5651 - val_acc: 0.8548 - lr: 2.0000e-04\n",
      "Epoch 28/1000\n",
      "262/262 [==============================] - 15s 56ms/step - loss: 0.0503 - acc: 0.9878 - val_loss: 0.5797 - val_acc: 0.8481 - lr: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "obsession_model = get_model()\n",
    "obsession_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "obsession_history = obsession_model.fit(padded_docs_train, y_train, epochs=1000, batch_size=16,validation_split=0.2,\n",
    "                    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss',patience=20, min_delta=1e-7),\n",
    "                              keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=10),\n",
    "                              keras.callbacks.ModelCheckpoint(filepath='model/lstm_obsession_enrich_model.h5', \n",
    "                                      monitor='val_loss', \n",
    "                                      save_best_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0bbd090-ad4d-4e23-b9b8-a6ddb56dcdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 1s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "obsession_model.load_weights('model/lstm_obsession_enrich_model.h5')\n",
    "obsession_predictions = obsession_model.predict([padded_docs_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34d9915a-2d6c-4c0c-8811-13fadc27bddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obsession Prediction Result\n",
      "Micro-average quality numbers for threshold 0.1\n",
      "Precision: 0.8709, Recall: 0.8709, F1-measure: 0.8709\n",
      "Micro-average quality numbers for threshold 0.2\n",
      "Precision: 0.8709, Recall: 0.8709, F1-measure: 0.8709\n",
      "Micro-average quality numbers for threshold 0.3\n",
      "Precision: 0.8762, Recall: 0.8762, F1-measure: 0.8762\n",
      "Micro-average quality numbers for threshold 0.4\n",
      "Precision: 0.8747, Recall: 0.8747, F1-measure: 0.8747\n",
      "Micro-average quality numbers for threshold 0.5\n",
      "Precision: 0.8724, Recall: 0.8724, F1-measure: 0.8724\n",
      "Micro-average quality numbers for threshold 0.6\n",
      "Precision: 0.8709, Recall: 0.8709, F1-measure: 0.8709\n",
      "Micro-average quality numbers for threshold 0.7\n",
      "Precision: 0.8617, Recall: 0.8617, F1-measure: 0.8617\n",
      "Micro-average quality numbers for threshold 0.8\n",
      "Precision: 0.8388, Recall: 0.8388, F1-measure: 0.8388\n",
      "Micro-average quality numbers for threshold 0.9\n",
      "Precision: 0.7815, Recall: 0.7815, F1-measure: 0.7815\n"
     ]
    }
   ],
   "source": [
    "print('Obsession Prediction Result')\n",
    "thresholds=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for val in thresholds:\n",
    "    pred=obsession_predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test.values[:, :1], pred, average='micro')\n",
    "    recall = recall_score(y_test.values[:, :1], pred, average='micro')\n",
    "    f1 = f1_score(y_test.values[:, :1], pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers for threshold\", val)\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d59d73d-de9b-4181-a9f5-206bc59b9a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = y_test.copy()\n",
    "Y_test['obsession_pred'] = obsession_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba3d3baa-d557-4b5d-82cb-a2d8cf59c915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obsession</th>\n",
       "      <th>obsession_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>1</td>\n",
       "      <td>0.989207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4982</th>\n",
       "      <td>1</td>\n",
       "      <td>0.973672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>1</td>\n",
       "      <td>0.988665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5835</th>\n",
       "      <td>1</td>\n",
       "      <td>0.988643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1675</th>\n",
       "      <td>1</td>\n",
       "      <td>0.986666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>0</td>\n",
       "      <td>0.025268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>0.891282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1</td>\n",
       "      <td>0.988762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>0</td>\n",
       "      <td>0.819443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3680</th>\n",
       "      <td>0</td>\n",
       "      <td>0.944341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>1</td>\n",
       "      <td>0.987881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>0</td>\n",
       "      <td>0.858445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3389</th>\n",
       "      <td>1</td>\n",
       "      <td>0.989357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>1</td>\n",
       "      <td>0.885545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>1</td>\n",
       "      <td>0.791317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>1</td>\n",
       "      <td>0.865559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>1</td>\n",
       "      <td>0.986552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2979</th>\n",
       "      <td>0</td>\n",
       "      <td>0.034614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1</td>\n",
       "      <td>0.986268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6460</th>\n",
       "      <td>1</td>\n",
       "      <td>0.988655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4584</th>\n",
       "      <td>1</td>\n",
       "      <td>0.989001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5174</th>\n",
       "      <td>1</td>\n",
       "      <td>0.989079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>1</td>\n",
       "      <td>0.897119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3067</th>\n",
       "      <td>1</td>\n",
       "      <td>0.696029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2133</th>\n",
       "      <td>0</td>\n",
       "      <td>0.668028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>1</td>\n",
       "      <td>0.986031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4626</th>\n",
       "      <td>1</td>\n",
       "      <td>0.988072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3691</th>\n",
       "      <td>1</td>\n",
       "      <td>0.890253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3585</th>\n",
       "      <td>1</td>\n",
       "      <td>0.981070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2310</th>\n",
       "      <td>1</td>\n",
       "      <td>0.987507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3101</th>\n",
       "      <td>1</td>\n",
       "      <td>0.977111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3460</th>\n",
       "      <td>1</td>\n",
       "      <td>0.988531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5434</th>\n",
       "      <td>1</td>\n",
       "      <td>0.989040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3935</th>\n",
       "      <td>0</td>\n",
       "      <td>0.033943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>1</td>\n",
       "      <td>0.974864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579</th>\n",
       "      <td>1</td>\n",
       "      <td>0.989487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3960</th>\n",
       "      <td>1</td>\n",
       "      <td>0.656565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3824</th>\n",
       "      <td>0</td>\n",
       "      <td>0.027031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>1</td>\n",
       "      <td>0.697923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1</td>\n",
       "      <td>0.987549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      obsession  obsession_pred\n",
       "1277          1        0.989207\n",
       "4982          1        0.973672\n",
       "1022          1        0.988665\n",
       "5835          1        0.988643\n",
       "1675          1        0.986666\n",
       "2137          0        0.025268\n",
       "48            1        0.891282\n",
       "751           1        0.988762\n",
       "1106          0        0.819443\n",
       "3680          0        0.944341\n",
       "2829          1        0.987881\n",
       "683           0        0.858445\n",
       "3389          1        0.989357\n",
       "3993          1        0.885545\n",
       "1284          1        0.791317\n",
       "2026          1        0.865559\n",
       "1460          1        0.986552\n",
       "2979          0        0.034614\n",
       "263           1        0.986268\n",
       "6460          1        0.988655\n",
       "4584          1        0.989001\n",
       "5174          1        0.989079\n",
       "2478          1        0.897119\n",
       "3067          1        0.696029\n",
       "2133          0        0.668028\n",
       "318           1        0.986031\n",
       "4626          1        0.988072\n",
       "3691          1        0.890253\n",
       "3585          1        0.981070\n",
       "2310          1        0.987507\n",
       "3101          1        0.977111\n",
       "3460          1        0.988531\n",
       "5434          1        0.989040\n",
       "3935          0        0.033943\n",
       "1668          1        0.974864\n",
       "5579          1        0.989487\n",
       "3960          1        0.656565\n",
       "3824          0        0.027031\n",
       "2559          1        0.697923\n",
       "495           1        0.987549"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86173202-4933-41ce-a597-01dd4da47d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('model/lstm_enrich_tockenizer_obsession.pkl', 'wb') as f:\n",
    "    pickle.dump(vect, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf4c90-3106-40ab-b650-40827d8e02a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
